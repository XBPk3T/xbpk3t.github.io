---
title: 《搞定系统设计：面试敲开大厂的门》
slug: /2024/system-design-interview
date:  2024-08-30
---


## 概览

这本书成书于2021年，还是比较新的，其中的系统设计题也确实是面试中比较高频的一些问题。

全书共17章

- 第1章：
- 第2章：系统的容量估算
- 第3章：***怎么回答系统设计类问题？***
- 从第4章到第7章：限流器、DHT、kvdb、分布式唯一id、
- 从第8章到第16章：短链模块、爬虫、通知模块、feed模块、聊天模块、搜索自动补全模块、视频分享模块、云盘、支付系统
- 第17章：


***其中尤其需要注意的是第三章，贯穿了全书，后面的所有系统设计，都是按照这个思维框架进行分析的。***

其实这个东西不稀奇，甚至很简单，我也早就知道，但是一直没有实践过。直到看到这本书全都按照这个框架进行分析，才知道确实没做好。



## 阅读笔记


### 3: 怎么回答系统设计类问题？

[系统设计面试题指北-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1745238) 这个post和本书里的流程一模一样


一共四步

***确定需求（具体功能、功能细节、QPS）、抽象设计（画简易架构图，具体来说就是确定架构的各组件）、常见优化点、结合该系统对优化进行实现***


***上面这个流程说起来简单，但是做起来需要很扎实的基础***（这可能也是为啥我一直实践不了这个流程的原因吧）

这里抛开不同系统的具体需求，只说通用需求

比如说 ***确定需求就需要封底估算，就需要知道常见性能指标，还要搞清楚这些性能指标通常对应着什么运营指标（也就是要知道性能指标和运营指标之间的换算）***

抽象设计，就需要了解各种组件，以及这些组件的性能指标


<details>
<summary>常用组件的性能指标</summary>

```markdown
然后我们需要了解一些常见的性能指标，这是我们需要记在脑子里的：

Nginx：能轻松的处理c100k问题，内存越大，能处理的并发量越高
Redis: https://redis.io/topics/benchmarks 表明，对于GET/SET来说，QPS 10-100k没啥大问题
MySQL: https://www.mysql.com/why-mysql/benchmarks/ 表明，对于只读，QPS 几百k没啥问题，对于写，MySQL 5.7 QPS 100k 几乎是上限
PG: https://www.percona.com/blog/2017/01/06/millions-queries-per-second-postgresql-and-mysql-peaceful-battle-at-modern-demanding-workloads/ 也是差不多
这些数据是不准确的，因为：

和怎么用关系很大
和硬件配置关系很大
但是我们心里还是要有个大概印象。
```

</details>

系统常见优化点则是通用的

<details>
<summary>系统常见优化点</summary>

```markdown
Step3:考虑系统目前需要优化的点
对系统进行抽象设计之后，你需要思考当前抽象的系统设计有哪些需要优化的点，比如说：

当前系统部署在一台机器够吗？是否需要部署在多台机器然后进行负载均衡呢？
数据库处理速度能否支撑业务需求？是否需要给指定字段加索引？是否需要读写分离？是否需要缓存？
数据量是否大到需要分库分表？
是否存在安全隐患？
系统是否需要分布式文件系统？
......

```

</details>








### 4: 限流器

这章分为两部分，功能和架构上实现限流

功能上，其实书中搞错了，这几种bucket算法都无法解决临界区的突发流量问题，也就是说如果限制 300reqs/sec, 那么在1s到2s中间其实可以发送600个req，***正确的做法应该是用redis lua实现，并且还天然支持分布式系统***



- [Scaling your API with rate limiters](https://stripe.com/blog/rate-limiters)
- [0-rate-limiters.md](https://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d)


```markdown
在设置好限流器之后，收集数据来检查限流器是否有效是很重要的。我们主要想确保：

•流量限制算法有效。
•流量限制规则有效。

举个例子，如果流量限制规则太严格，就会导致很多有效请求被丢弃。在这种情况下，我们希望稍微放宽限制。另一个例子是，我们发现，在限时促销这种流量激增的场景下，限流器变得无效了。因此，可能需要换一种流量限制算法来应对突发的流量。这时候，代币桶就是一个合适的替代算法。
```




架构上，书中抛出了一个问题，***在分布式系统中怎么使用限流器？***

很简单，hystrix 之类的circuit breaker本身就支持限流器

那么，***怎么实现一个简易的分布式系统的流量控制服务？***





### 8: 短链服务


```markdown
写操作：每天生成1亿个URL。

•每秒的写操作数：1亿÷24÷3600≈1160。

•每秒的读操作数：假设读操作与写操作的比例是10∶1，那么每秒的读操作数是1160×10=11，600。

•假设URL缩短器会运行10年，这意味着我们必须支持1亿×365×10=3650亿条记录。

•假设URL的平均长度是100个字符，那么10年的存储容量需求是：3650亿×100字节≈36.5TB。

与面试官一起审查这些假设和估算很重要，这样能确保你们俩对系统需求有相同的理解。
```

需要注意保存时间，关系到存储成本


### 9: 爬虫

```markdown
下面的估算基于很多假设，与面试官交流并达成共识很重要。

•假设每个月要下载10亿个网页。

•QPS：1，000，000，000÷30÷24÷3600≈400，即每秒约400个网页。•峰值QPS=2×QPS=800。

•假设平均每个网页的大小是500KB。

•每月需要存储1，000，000，000×500KB=500TB。如果你不太熟悉存储单位的含义，请重新阅读第2章的2.1节。

•假设数据要保存5年，则500TB×12×5=30PB，即需要30PB的存储空间来保存5年的内容。
```


```markdown
你可以把网络想成一个有向图，其中网页就是节点，超链接(URL)是边。爬虫在网络上爬行的过程可以看作是从一个网页到其他网页的有向图遍历。常见的两种图遍历算法是DFS和BFS。但是，因为DFS的深你可以把网络想成一个有向图，其中网页就是节点，超链接(URL)是边。爬虫在网络上爬行的过程可以看作是从一个网页到其他网页的有向图遍历。常见的两种图遍历算法是DFS和BFS。但是，因为DFS的深度可能非常深，所以它通常不是一个好的选择。

BFS是爬虫常用的方法，通过先进先出(FIFO)队列来实现。在一个FIFO队列中，URL按照它们入列的顺序出列。尽管如此，这种实现方式还有以下两个问题。


•同一个网页的大部分链接都指向同一个主机。如图9-5所示，wikipedia.com中的所有链接都是内部链接，这使得爬虫忙于处理来自同一个主机(wikipedia.com)的URL。当爬虫尝试并行下载网页时，维基百科的服务器会被大量请求“淹没”​。这样做被认为是“不礼貌”的。

•标准的BFS并没有考虑URL的优先级。互联网很大，不是每个网页都有同样水平的质量和同等重要性。因此，我们可能想要基于网页的排名、网络流量、更新频率等对URL进行排序，以便优先处理某些网页。

```

```markdown
9.3.6 检测和避免有问题的内容本节讨论检测及避免重复、无意义或者有害内容的方法。


蜘蛛陷阱

蜘蛛陷阱是可以导致爬虫陷入无限循环的网页，例如，一个无限深的目录结构www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/…。可以通过设置最大URL长度来避免这样的蜘蛛陷阱。尽管如此，并不存在检测蜘蛛陷阱的通用解决方案。含有蜘蛛陷阱的网站是容易识别的，因为在这种网站上网页的数量异常多。但是很难开发出一个自动算法来躲避蜘蛛陷阱。不过，用户可以手动验证和识别蜘蛛陷阱，然后要么在爬取时排除这些网站，要么应用一些定制的URL过滤器。
```

爬虫陷阱



### 12: IM模块





### 13: 搜索建议模块

```markdown
当我们在谷歌上搜索或者在亚马逊上购物时，只要在搜索框中打字，网页上就会展示一个或者更多的与搜索词匹配的结果。这个功能叫作自动补全(Autocomplete)、提前输入(Typeahead)、边输边搜(Search-as-you-type)或者增量搜索(Incremental Search)

这引出了我们的面试问题：设计一个搜索自动补全系统，也即设计一个能展示“Top k查询词”或者“k个最常被搜索的查询词”的系统。
```

搜索建议都是有时效性的

只不过不同应用的时敏不同而已，比如weibo, twitter, tiktok之类的社交媒体平台时敏就更高，也就是说时间权重更高

而google之类的搜索引擎则 search frequency的权重则高于 时效



```markdown
(1)如何扩展你的设计来支持多语言？

为了支持非英文的查询词，我们在字典树节点中存储Unicode字符。如果你不熟悉Unicode，这里介绍一下它的定义：​“一个涵盖世界上所有书写系统的所有字符的编码标准，无论是现代还是古代的书写系统。​”欲了解更多的内容，请访问Unicode的官网。



(2)如果某个国家的高频查询词与其他国家的不一样怎么办？

在这种情况下，我们可能要为不同国家构建不同的字典树。为了提升响应速度，我们可以把字典树存储在CDN中。



(3)如何支持趋势性（实时）查询词？

假设爆发了一个新闻事件，一个查询词瞬间变得流行起来。我们原先的设计并不能支持这种情况，这是因为：

•原定每周更新字典树，所以下线的Worker并不会立即更新字典树。

•即使正好是预定这个时间更新字典树，也需要花很长的时间来创建字典树。构建一个实时搜索自动补全系统是很复杂的，它不在本书的讨论范围内，这里我们只会提供一些思路：
•通过分片来缩减工作数据集的大小。
•改变排序模型，给最新的查询词分配更高的权重。
•数据可能是以流的形式进入系统的，所以我们无法一次访问所有的数据。

流数据意味着数据是持续生成的。流数据的处理需要一组不同的系统：Apache Hadoop MapReduce、Apache Spark Streaming、Apache Storm、Apache Kafka等。所有这些话题都涉及特定的领域知识，因此这里不会讨论它们的细节。
```

峰值QPS为QPS×2


[峰值QPS为QPS×2 - Google Search](https://www.google.com/search?q=%E5%B3%B0%E5%80%BCQPS%E4%B8%BAQPS%C3%972)









### 14: youtube

```markdown
CDN成本。当由CDN来提供视频服务时，我们要为从CDN传输出去的数据付费。我们使用亚马逊的CDN CloudFront来进行成本估算，图14-1列出了数据传输到互联网的按需收费价格（每GB的价格，单位为美元）​。假设100%的流量都来自美国，平均每GB的价格是0.02美元。为了简单起见，我们只计算视频流服务的成本。

500万×5个视频×0.3GB×0.02美元=150，000美元

根据这个粗略的成本估算，我们发现通过CDN来提供视频要花很多钱。即使云服务提供商愿意为大客户降低CDN成本，但这个费用还是很高。我们会在14.3节中谈论降低CDN成本的办法。
```


```markdown
之前已经讨论过，面试官建议使用已有的云服务而不是从头构建所有的东西。CDN和Blob存储是我们将会用到的云服务。有些读者可能会问，为什么不自己构建所有服务呢？原因如下：

•系统设计面试不要求我们从头开始构建一切。在有限的面试时间里，选择正确的技术来正确地完成工作比详细解释技术的原理更重要。举个例子，对于面试来说，提到用Blob存储来存储源视频就足够了。要是谈论Blob存储的详细设计可能就有点画蛇添足了。

•构建一个可扩展的Blob存储或者CDN是极其复杂和昂贵的。即使像Netflix或者Facebook这样的大公司也没有自己构建所有的东西。Netflix使用了亚马逊的云服务，Facebook使用Akamai的CDN。
```

blob存储 其实就是类似 OSS, COS 之类的 object storage服务




### 15: 云盘

### 16: 支付系统



### 17: 设计指标监控和告警系统










## 总结

这本书的内容还是很实用的，对我来说有点浅了，但是查缺补漏嘛，还是学到了不少东西，比如：

- 应该是 封底估算，而不是 容量估算

[封底估算(Back-of-the-Envelope Estimation) | lijianfei.com](https://lijianfei.com/post/feng-di-gu-suan-back-of-the-envelope-estimation/#:~:text=Back%2Dof%2Dthe%2Denvelope%20estimation%EF%BC%8C%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%BA,%E4%BC%B0%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E3%80%82)

back of the envelop estimation =


封底估算 通常涉及哪些方面？

***通常是先确定需求，计算QPS，再根据读写比，得出读写操作各自的QPS。再假设该服务运行时间，计算存储成本。***











## [2023/11/13] 短链接

:::tip

*“短链接服务”的核心需求？有哪些发号器算法？*

细节：映射关系？为什么要用 302 跳转？失效检测怎么做？

:::




### 核心需求

- 短链接生成（发号器）
- 失效检测：自动遍历所有链接，判断是否已失效
- 关键词检测：设置关键词，用于判断网页内是否包含此关键词，以判断是否失效
- 短链统计：转化排行榜（转化效果评估）


### 为什么要用短链，而不是长链接？

- 在限制内容长度的平台如短信、微博，可以有效控制
- url 转二维码，最好使用短链，因为长链生成的二维码更密集，不利于传播
- 有些平台无法识别长链，只截取部分 url，会产生问题
- 可以统一管理所有 url，三方链接存在于多个网页/代码中，一旦变更，就需要多处更改


### 发号器

短链接服务的`发号器`怎么实现？生成 url 的算法有哪些？怎么选择？



- `类 uuid`（插入 db 时可能会频繁导致页分裂和不饱和的节点，导致数据库插入性能降低，影响插入性能）
- `snowflake`（snowflake 依赖于系统时钟的一致性，如果某台机器的系统时钟回拨，有可能造成 ID 冲突，或者 ID 乱序）
- *`MurmurHash 算法`+`转进制`*，MurmurHash 的 32bit 的最大值是 43 亿，10 进制转 62 进制之后，6 位数的 62 进制大概 568 亿
- *不希望反推出全局 ID，使用洗牌算法*，打乱算出的值，比如十进制的 201314 就可以转换为 Qn0。然后再使用洗牌算法，可以返回 n0Q、Q0n....其中之一。但是会有一定几率冲突，多洗几次就行
- 希望反推出全局 ID；那就在得到 Qn0 这个数字后，将其转换为二进制数。然后在固定位，第五位，第十位...(等等) 插入一个随机值即可。至于如何反推也很简单，你拿到短链接 key 后，将固定位的数字去除，再转换为十进制即可。

---

- 使用哈希算法，在短链接场景下，怎么避免哈希冲突呢？
  - 根据布隆过滤器*如果返回不存在则一定不存在*的特性，进行反查，或者直接使用哈希冲突概率更低的`布谷鸟过滤器`
- 使用什么存储介质来保存短链？用 redis 存，往 db 里持久化一份就可以了，不要通过 db 去查询


[How to generate a random string of a fixed length in Go? - Stack Overflow](https://stackoverflow.com/questions/22892120/how-to-generate-a-random-string-of-a-fixed-length-in-go)



### 细节

- 短链接和长链接的映射关系？一对一还是一对多？为什么？
- 301 还是 302？为什么要用 302 跳转？
- 短链接的失效检测？

---

- *一对多*，同一个长链接，因为不同的参数，所以生成不同的短链接
- 可以通过这些参数做数据分析，比如用户相关参数如`生成 url 的用户名`、`当前的 ua`、`之前的 referer`等参数





### 301 还是 302？为什么要用 302 跳转？

*一定要用 302 临时重定向，主要是基于`浏览器缓存问题`，以及由他产生的（坏链、统计和可控问题）三个问题*

- `坏链`，长链坏链了，我们修改之后，301 跳转还是会跳到坏链
- `短链可控问题`，如果部分长链的内容涉嫌违规，使用 301 跳转无法实现
- `统计问题`，使用 301 跳转，直接走缓存，服务端无法统计该 url 的真实使用次数







### 服务优化

怎么优化短链接服务？

- 分库分表
- 读写分离
- 引入缓存
- 可以修改*全局发号器*的算法
- 防攻击


