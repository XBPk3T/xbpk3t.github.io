---
title: 《趣谈 Linux 操作系统》读书笔记
date: 2023-9-17
slug: /2023/kernel
description: 2024-09-29从kernel.yml整理出来的
---




## kernel




### micro, macro, hybrid


```yaml

- 单内核: 也就是企业文化，单内核就是“大政府” （“经营企业的方式也有很多种，到底是人性化的，还是强纪律的”，或者说 单内核、微内核还是混合内核）

#懂了，我说说我的理解，你看对不对哈。微内核虽然把很多原本应该在内核的权限下放到用户空间，理论上会减少切换，但是用户空间之间的通信还是要通过内核，原本在内核内部不需要切换的工作，现在反而需要切换了。还是拿公司举例，内核就是公司老板，原本公司有 10 个高管，各管一摊，但是现在为了精兵简政，提高效率，把其中的 7 个高管降级了，那么原本这 10 个高管开个会就能解决的事，现在 3 个人就拍不了板了，需要频繁跟那 7 个被降级的高管沟通。站在这 7 个高管的角度，也很难受，本来在内部一句话的事，现在互相沟通还要经过那 3 个高管，效率就变低了。不知道我理解的对吗？

#懂了，这么说的话，拿政府举例，单内核就是大政府，里面什么都有，沟通方便，但是缺点是叠床架屋，并且组件太多，牵一发而动全身，本身存在一定的稳定性问题，“无限责任制政府”嘛。微内核就是小政府，只保留核心权力，其他的都分出去，能够保证内核自身稳定，但是因为也分出去了一些很重要的权力，虽然能够提高灵活性，但是会导致大量需要与政府勾兑的事情，不仅降低效率，还会浪费空间。混合内核就是一种比较理想的状态，权责划分合理，效率和稳定性都很高，但是很难实现，就像理想国一样，人类历史上还没出现过这玩意。

```


```yaml
API: 公司产品说明书（会经常修改）

ABI: 可以类比为公司的基本信息和运营参数，如公司的地址、规模、性质、电话号码等。这些信息基本上不可能修改。ABI保证了userspace能正确与kernel进行交互，也正如企业固定地址、固定号码，方便其他公司跟本公司洽谈业务。

# - **公司的地址** 对应于内核的系统调用接口，它是一个固定的“地点”，开发者和用户空间程序可以通过这个接口与内核进行交互。
# - **公司的规模** 类似于内核支持的功能集和性能参数，它决定了公司能处理的业务量和复杂度。
# - **公司性质** 对应于内核的类型和设计目标，比如是面向企业服务的还是面向消费者市场的。
# - **公司电话号码** 类似于内核提供的系统调用编号，每个编号都对应一个特定的服务或功能。

# 内核 ABI 通过定义一套稳定的接口和调用约定，确保用户空间的程序能够可靠地与内核进行通信和交互。这些接口和约定是操作系统兼容性和稳定性的关键，它们使得开发者可以确信，他们的应用程序能够在内核的支持下正常运行，即使在内核版本更新后也能保持兼容性。

```



### 内核态和用户态


在内核态，CPU 拥有完全的权限，可以执行特权指令和访问系统的所有内存。这意味着操作系统可以执行任何操作，包括修改内存中的敏感数据和控制硬件设备。而在用户态，CPU 的权限被限制在非特权指令和受限内存访问上。这样做是为了保护系统的安全性和稳定性，防止应用程序滥用系统资源或干扰其他应用程序的正常运行。

```yaml
KernelMode: ring0(direct),
UserMode: ring3(syscall)

kernel-space: 公司内部
# DMA, NORMAL, HIGHMEM
user-space: 外包（相应的，外部设备就是其他公司）
# text section, data section, bbs segment, heap, stack

# BIOS（或者 UEFI）就是政府（硬件）开公司的流程（BIOS/UEFI 固件直接嵌入计算机主板），CPU 就是员工，文件系统就是公司的档案库，，0 号进程就是项目模板，进程树就是项目树（比如公司项目内部嵌套的各种项目）

CPU: 员工，或者说工人。但是在kernel中，通常按照单核和多核来说，或许类比成“多手工业机器人”更合适，因为CPU只能看懂 ELF，需要有人把客户需求转化成项目执行计划书（ELF），CPU 小伙伴们才能执行，这就是编译。
# 编译其实是一个需求分析和需求转换的过程。这个过程会将接近人类的 C/C++ 语言，转换为 CPU 小伙伴能够听懂的二进制语言，并且以一定的文档格式，写成项目执行计划书。
```

```markdown
如果我们还拿公司举例，把用户态线程类比为职员，把内核态线程类比为高管，是不是可以认为，“内核态和用户态是一种运行模式”可以理解为，如果让这个职员负责高管的工作，他现在的工作模式就是内核态，如果他只做普通的执行工作，他的工作模式就是用户态。跟他本身无关，只不过是内核调度他进行不同的工作模式（公司老板让他负责不同的工作）
```

其实回过头看，这个比喻是有问题的。

下面这个进程类比成公司正式立项的项目，线程类比成项目内部子项目更合理。




### syscall & glibc



```yaml
syscall: syscall 是用户空间调用内核指定操作的方法。syscall 可以看作是能执行指定操作的秘书，用来连接两者（syscall 决定了操作系统好不好用，功能全不全）。

glibc: 虽然 syscall（秘书的指令表）已经很方便了，但是只是对公司内部（）而言，对公司外部的人对这个肯定不熟啊，就需要 glibc 了。glibc 封装了 linux 的 syscall，方便调用。
# 但是需要注意的是，并不是glibc一定封装了多个syscall，可能是1:1
# 每个特定的系统调用对应了至少一个 Glibc 封装的库函数，比如说，系统提供的打开文件系统调用 sys_open 对应的是 Glibc 中的 open 函数。
#  有时候，Glibc 一个单独的 API 可能调用多个系统调用，比如说，Glibc 提供的 printf 函数就会调用如 sys_open、sys_mmap、sys_write、sys_close 等等系统调用。
#  也有时候，多个 API 也可能只对应同一个系统调用，如 Glibc 下实现的 malloc、calloc、free 等函数用来分配和释放内存，都利用了内核的 sys_brk 的系统调用。
```

syscall是公司内部的秘书，glibc是中介机构

一对一、一对多、多对一



## CPU


### z

```yaml
# 多核 CPU 一致性问题

- 内存屏障:
# 内存屏障是一种指令，用于限制指令重排和内存访问的顺序。它可以确保在屏障之前的指令和内存访问在屏障之后的指令和内存访问之前完成，从而保证程序的正确执行顺序。

- MESI(Modified, Exclusive, Shared, Invalid):
# MESI 是一种缓存一致性协议，用于多处理器系统中的缓存一致性。它定义了处理器缓存中数据的四种状态：修改（Modified）、专用（Exclusive）、共享（Shared）和无效（Invalid）
# The MESI protocol is an Invalidate-based cache coherence protocol, and is one of the most common protocols that support write-back caches.

- 指令重排:
# 指令重排是编译器或处理器在执行程序时重新排序指令的过程，以优化性能或提高并发性。然而，在多线程环境中，指令重排可能会导致意想不到的结果，如数据竞争或不一致的状态。

- Happens-Before:
# Happens-Before 原则是指在多线程编程中，如果一个操作的结果对于另一个操作是可见的，那么第一个操作在时间上发生在第二个操作之前。Happens-Before 原则是用于定义和推断多线程程序中操作的顺序关系，以确保正确的执行顺序和一致性。

- volatile 关键字:
# volatile 是一个关键字，用于修饰变量，指示编译器在生成指令时不要对该变量进行指令重排。它可以确保对 volatile 变量的读写操作按照程序的顺序执行，并且对于其他线程可见，从而提供了一定的内存可见性和顺序性保证。
```

我说说我的理解，还是拿上面老板和工人来举例，指令重排就是工人为了尽快完成工作，自己对工作任务的优先级进行调整，这种情况下，如果多人协作就会出问题。为了解决这个问题，“volatile 就通过内存屏障来禁止某些情况下的指令重排”，如果我们把内存屏障理解为公司规章制度，volatile 理解为公司的通知板，就可以理解为工人的前后操作，如果其中一个是普通读写（以及第二个操作是 volatile 读）都可以重新排序，用来提高效率。其他情况（比如有 volatile 写）会导致数据不一致情况发生的，就需要禁止指令重排。我理解的对吗？

---

你的理解基本上是正确的。指令重排是为了提高工作效率，但在多线程环境下可能会导致数据不一致的问题。volatile 关键字通过内存屏障来禁止某些情况下的指令重排，确保数据的一致性。你可以将内存屏障理解为公司的规章制度，volatile 关键字理解为公司的通知板，工人的前后操作可以重新排序，但是如果其中一个操作是 volatile 读，就不能进行重新排序，以确保数据的正确性。当涉及到 volatile 写操作时，更需要禁止指令重排，以避免数据不一致的情况发生。总的来说，你的理解是正确的。

“当一个核心执行一个写操作，并且另一个核心执行一个读操作时，happens-before 关系可以用来确保写操作先于读操作发生。这意味着读操作可以看到写操作所做的更改。”这个就是内存可见性是吧

---

是的，您提到的情况涉及到内存可见性。内存可见性是指当一个线程对共享变量进行了修改后，其他线程能够立即看到这个修改。在多核 CPU 中，每个核心都有自己的缓存，当一个核心对共享变量进行写操作时，它会将修改写入自己的缓存中。而其他核心在读取该共享变量时，可能会从自己的缓存中读取，而不是从主内存中读取。这就可能导致一个核心看到的是旧值，而另一个核心看到的是新值。为了确保内存可见性，需要使用 happens-before 关系来进行同步。happens-before 关系可以确保写操作先于读操作发生，从而保证读操作可以看到写操作所做的更改。


### CPU上下文


registers(..., PC, stack pointer), process status, mem-manage(page table, segment table)

```markdown
CPU 上下文切换的过程如下：
1. 当操作系统决定切换到另一个进程时，它会保存当前进程的 CPU 上下文。这包括将寄存器的值保存到当前进程的内存区域中。
2. 操作系统根据调度算法选择下一个要执行的进程，并从其内存区域中恢复其先前保存的 CPU 上下文。
3. 恢复后，操作系统将控制权转移到新进程的程序计数器（PC）指向的位置，从该位置继续执行。

CPU 上下文切换是一项开销较高的操作，因为涉及到保存和恢复大量的状态信息。它发生在多任务操作系统中，用于实现进程间的切换和资源共享，以便为每个进程提供公平的 CPU 时间片，并使系统能够同时运行多个任务。
```

***CPU的让步式上下文切换和抢占式上下文切换，不如说是主动调度和被动调度***

按照我的理解，线程的调度机制，被动调度就是抢占式调度，就是 work stealing 机制。主动调度就是主动让出资源，就是 hand off 机制。看起来进程和线程的调度机制没啥区别，进程和线程从调度算法到上下文切换到调度机制，实际上都没有区别，我理解的对吗？

---

```markdown
我认为把TCB类比成公司二级项目的项目经理很贴切，那么能否用这个类比给我描述一下，整个thread切换上下文的过程？
---

使用公司一级项目项目经理的类比来描述进程上下文切换的过程，我们可以这样想象：

在一个大型公司中，有多个一级项目同时进行，每个项目都有自己的项目经理（类比为PCB，即进程控制块）。这些项目经理负责确保他们各自的项目按照计划和目标推进。当需要在不同项目之间切换注意力时，公司会进行一系列的步骤来确保平滑过渡，这个过程类似于操作系统中的进程上下文切换。

1. **保存当前状态**: 当公司决定从一个项目（进程）切换到另一个项目时，当前项目的项目经理需要记录下项目的所有关键信息。这包括项目进度、下一步计划、当前遇到的问题、团队成员的任务分配等。在进程切换中，操作系统会保存当前进程的状态到其PCB中，包括程序计数器、寄存器状态、内存分配、打开的文件描述符等。

2. **评估和优先级**: 公司会根据项目的紧急程度、截止日期和资源需求来决定下一个要关注的项目。类似地，在操作系统中，进程调度器会根据进程的优先级、调度策略和其他因素来选择下一个要运行的进程。

3. **资源分配**: 确定下一个项目后，公司会重新分配必要的资源，如资金、人力和其他物资。在进程切换中，操作系统会确保新选中的进程有足够的资源来运行，包括内存分配、I/O权限等。

4. **加载新状态**: 接下来，公司将注意力和资源转移到新的项目上，新的项目经理会更新自己的工作状态，准备开始新的工作。在操作系统中，进程调度器会从新选中的进程的PCB中加载状态信息，恢复进程的执行环境。

5. **执行新任务**: 最后，新的项目经理（进程）开始执行他们的任务，推进项目进度。在操作系统中，新进程开始执行其代码，直到完成或再次被操作系统挂起。

通过这个类比，我们可以更直观地理解进程上下文切换的过程。就像公司在不同项目经理和项目之间进行切换一样，操作系统在不同的进程之间进行上下文切换，以确保系统的多任务和并发能力。这个过程需要仔细的规划和管理，以确保每个进程（项目）都能高效地运行。
```

thread切换和process切换（除了设计的资源和开销不同以外）整个流程完全一致，



### irq 和 softirq


```markdown
Signals are similar to interrupts, the difference being that interrupts are mediated by the CPU and handled by the kernel while signals are mediated by the kernel (possibly via system calls) and handled by individual processes.[citation needed] The kernel may pass an interrupt as a signal to the process that caused it (typical examples are SIGSEGV, SIGBUS, SIGILL and SIGFPE).
```

- interrupts mediated by CPU, handled by Kernel
- signal mediated by kernel, handled by process.

---

- 当一个硬件中断发生时，首先执行的是硬中断处理，这部分处理需要尽可能快，以避免影响系统对其他中断的响应。
- 一旦硬中断处理完成，如果还有更多的工作需要完成，这部分工作会通过软中断来异步处理。这样，即使处理过程耗时较长，也不会阻塞硬件中断的处理。


---


灵活的待办事项或低优先级的背景任务。这些任务不需要立即完成，但仍然对项目的长期成功和维护有积极作用。

用于处理可以延迟执行的任务，以避免阻塞更重要的中断处理

除了signal，还有哪些“软中断”? # syscall, exception, traps, page faults






## process



### vs


```yaml


- 子进程: 公司二级项目（公司项目下的子项目）。它们是从主项目中派生出来的，有自己的特定任务和目标，但仍然属于同一个大项目。子进程继承了父进程（主项目）的一些资源和属性，但也可以独立地执行任务。
- 线程: 可以类比为项目中的工作小组或者单个任务。在一个项目（进程）中，可以有多个线程（工作小组）并行工作，每个小组负责一个具体的任务。这些小组共享项目的资源，但可以独立地执行工作，提高项目的完成效率。
- 协程: 该工作小组下的某个功能开发（或者某个外包项目）
- LWP: 临时项目
# LWP是介于进程和线程之间的概念，它提供了进程和线程之间的一种折中方案。LWP 拥有自己的地址空间，但在某些实现中可能与其他 LWP 共享某些资源。LWP 通常用于实现多线程，但它们的调度和管理类似于进程。LWP 的上下文切换开销介于进程和线程之间。LWP 通常是进程内的一个执行单元，它们的创建、销毁和管理比进程更快速和灵活。

---

- task_struct(tgid, flag)
- scheduler(kernel, (pthread?))
- communication(IPC, ITC)
- state(RWSZNX, RWBTRT, zombie, blocked)
- context switching overhead(PCB/TCB, Register(?, 16, 3), time(?, 1000ns, 200ns))
- mem-use(?, 1MB, 2KB)
- create/destroy(kernel, runtime)

```

---

我知道进程和线程的struct都是task_struct，但是我确实并不清楚“线程创建过程的具体流程”

```markdown
可见和创建进程时使用的 fork 系统调用相比，创建线程的 clone 系统调用几乎和 fork 差不多，也一样使用的是内核里的 do_fork 函数，最后走到 copy_process 来完整创建。

不过创建过程的区别是二者在调用 do_fork 时传入的 clone_flags 里的标记不一样！。

- 创建进程时的 flag：仅有一个 SIGCHLD
- 创建线程时的 flag：包括 CLONE_VM、CLONE_FS、CLONE_FILES、CLONE_SIGNAL、CLONE_SETTLS、CLONE_PARENT_SETTID、CLONE_CHILD_CLEARTID、CLONE_SYSVSEM。

关于这些 flag 的含义，我们选几个关键的做一个简单的介绍，后面介绍 do_fork 细节的时候会再次涉及到。

- CLONE_VM: 新 task 和父进程共享地址空间
- CLONE_FS：新 task 和父进程共享文件系统信息
- CLONE_FILES：新 task 和父进程共享文件描述符表
```

创建进程和线程分别使用fork和clone的syscall，但是实际上两个都是通过do_fork来调用copy_process来进行创建。

*两者的区别就在于do_fork的flag不同*

还是拿公司项目举例，process就是项目立项了，但是在项目内部，需要实现不同的模块时，就需要不同的flag，所以线程更多元化一些。

通过 copy_process 的源码可以看到，就是两部分，从idle process复制

---
总结一下：

我们还是拿进程比做公司项目，线程和协程比做大项目下的子项目和某个功能的开发。

进程调度和线程调度从调度算法、上下文切换、调度机制等方面其实都没啥区别。比如说，调度算法都是 FIFO、SJPF、CFS、MLFQ、轮询之类的（本质上来说是 CPU 调度算法，而不是什么进程或者线程的调度算法），上下文切换也是类似的，调度机制也是类似，都是主动调度和被动调度两种。

但是一定需要注意的是，两者最本质的区别就是，进程是抢夺 CPU，线程则是抢夺进程（最终抢夺 CPU），（协程抢夺线程）。

项目之所以要分成多个子项目组，不也是为了能够并发执行吗？同样的，如果子项目一旦阻塞（没完成任务），那你整个项目（进程）就阻塞了。那么用户态线程就类似外包项目组，更灵活，随时可以创建、销毁，开销很低，并且即使阻塞，也不会导致整个进程阻塞。

---

这么说的话，我想到一个非常好的类比。进程可以类比成有公司立项的项目，分配了各种独立资源，线程则可以类比成这个项目中的未立项（也不需要立项）的子项目，依托于这个项目存在，这两个都是项目（数据结构都是 task_struct），但是公司的资源只能划拨给已立项的项目（进程），但是实际上在项目内部实际执行操作的是他创建的各种子项目（线程）。这个比之前那个公司部门和工人的类比要贴切太多了。

其实挺简单的，每个process的用户地址空间都是独立的，不能互相访问。但是kernel space是所有process共享的，而process之间也是通过kernel来通信的。

而process内又是所有thread共享的。举个例子就是，公司的一级子公司（或者说项目组）都必须要经过总公司通过公司老板（kernel）来传递消息，来进行通信。而这些一级项目组内部的这些项目组，则可以直接通信，不需要kernel了。

另外，还有一点非常贴切的是，不管是一级项目组还是二级项目组，都是要在公司里立项的，接受公司的调度。但是资源只发到一级项目组，又一级项目组对内部资源进行分配。如果一级项目组被撤项了，所有资源肯定就收回了。连带着里面的各种二级项目组都挂了。

---

```markdown
除了fork函数之外，还有另外几个提供给用户态用于创建进程相关的函数。

vfork：与fork的区别在于，vfork保证子进程先运行，在它调用exec或exit之后父进程才可能被调度运行。
clone：pthread函数族使用clone来创建轻量级进程。
```


### 告诉我 process 派生 child process 的整个流程？


父进程通过`fork()`创建子进程，子进程继承了父进程的资源和地址空间。子进程可以选择调用`exec()`来加载并执行新的程序，或者继续执行当前的程序。当父进程或子进程通过`exit()`终止时，操作系统会回收它们占用的资源。

```markdown
fork：fork函数拷贝父进程来创建一个子进程，fork函数调用后会分别在父子进程中各返回一次，区别在于：父进程中的返回值是所创建的子进程的进程pid，而子进程则是返回0表示创建成功。
exec函数组：在fork调用返回后，子进程就创建完成了，如果需要运行一个与父进程不同的可执行文件，就通过exec函数组来完成这个工作。如果不调用exec，那么也就意味着父子进程运行的是同一份可执行文件代码。

...

前面提到过，fork函数为子进程创建一个父进程地址空间的副本，复制属于父进程的页面。然而，考虑到许多子进程在创建之后立即调用系统调用exec函数组来执行另外的程序代码，父进程地址空间的复制可能没有必要。

因此，Linux内核在实现时，使用了写时复制的技术（Copy On Write，简称COW），子进程在刚开始创建时与父进程共享同样的地址空间，仅当子进程要修改父进程地址空间的内容时才创建新的地址空间，从而使父子进程有各自的拷贝。

Redis在生成RDB快照文件时，就利用了Linux写时复制机制。生成快照文件时，Redis主进程fork创建一个子进程，根据这里的解释，此时子进程的内存地址就是共享的父进程的空间。这样，父进程可以继续服务请求，而子进程跟进进程创建时候的内存信息，生成快照文件，结束了之后自行退出即可。
```

fork+exec就是派生child process的过程

包括COW，说白了就是让child process使用父进程的地址空间。




### process state, scheduler



```yaml
- 进程: 公司项目
# 在操作系统中，每个进程都有自己的内存，互相之间不干扰，有独立的进程内存空间。
# task_struct中的各种filed类比成什么比较合适?
# task_struct # state, pid, mm(ms_struct), fs(fs_struct), files(files_struct)

# 项目状态? RWSZ
# 项目排期? scheduler
# 项目运行的统计信息也非常重要，项目运行过程中占用的公司的资源，例如会议室（内存）、档案库（文件系统）也需要在项目管理系统里面登记。
# process lifecycle: process create, start and exit?

```

---

相比于部门的类比，对进程/线程/协程更恰当的类比是项目组。因为部门没办法随时创建，随时销毁。

并且如果用部门来类比，也没办法类比idle process, init process, kthreadd process这些

---

通过这个例子就可以很简单地解释了，为什么只有在系统调用、异常、中断的时候，才需要用户态切换到内核态。在公司里不也是遇到解决不了的问题了，或者需要更高权限的时候，就需要给公司部门老大汇报？




### process scheduler



```yaml
- 进程调度: 这个场景也很好理解，就像我们平时工作时，打开 kanban 也是一堆子项目（一个 CPU 对应一堆进程），那肯定要先想想到底先搞哪个项目才是最优选择是吧。内核对进程的调度，就像是公司对不同项目的优先级安排是不同的。
# 这里就涉及到各种scheduler algo了嘛


# 对于操作系统来讲，他面对的 CPU 的数量是有限的，干活儿都是他们，但是进程数目远远超过 CPU 的数目，因而就需要进行进程的调度，有效地分配 CPU 的时间，既要保证进程的最快响应，也要保证进程之间的公平。
# 一个 CPU 上有一个队列，队列里面是一系列 sched_entity，每个 sched_entity 都属于一个 task_struct，代表进程或者线程。
# 调度要解决的第一个问题是，每一个 CPU 小伙伴每过一段时间，都要想一下，白板上这么多项目，我应该干哪一个？CPU 的队列里面有这么多的进程或者线程，应该取出哪一个来执行？

# kernel中父进程在fork创建子进程时的具体流程?

```

sched_class (dl/rt/fair/idle_sched_class)

scheduler, strategy, algo

linux进程调度四要素（核心调度器、就绪队列rq、调度算法、进程优先级nice）

```markdown
weight = 1024 / (1.25 ^ nice)

公式中的1.25取值依据是：进程每降低一个nice值，将多获得10%的cpu时间。公式中以1024权重为基准值计算得来，1024权重对应nice值为0，其权重被称为NICE_0_LOAD。默认情况下，大部分进程的权重基本都是NICE_0_LOAD。
```

进程每降低一个nice值，将多获得10% CPU的时间

process scheduler 调度算法

在Linux内核中，进程调度算法通常采用CFS，它基于红黑树实现。CFS旨在提供公平的CPU时间分配，并根据进程的优先级和历史执行时间进行动态调整。kernel的process scheduler就4种，CFS、RT、O(1).





### orphan, zombie. 我们都知道orphan是因为ppid没了，zombie是因为ppid废了。那什么情况下ppid会没了或者废了呢?



先分清楚pid和ppid

orphan就是没有ppid的pid，孤儿嘛。所以怎么处理呢？kernel会把orphan的ppid设置为init，让init去处理orphan

zombie则是虽然有ppid但是实际上废了，所以怎么处理呢？当然是先把zombie的ppid干掉，这些pid就变成orphan了，之后就按照处理orphan的流程处理掉就ok了

还是拿项目举例，孤儿进程就是我们某个已经立项的大项目挂掉了，下面的子项目就直接由公司的顶级项目接手并收尾。



### idle process, init process, kthreadd process



```yaml
- idle process: 公司的行政部门（0号进程是内核的一部分，通常被称为 "swapper" 或 "scheduler" 进程，它负责管理进程的调度和切换。）（因为0号、1号、2号进程都无法被kill掉，都是“常设项目”，可以被认为是部门）
# idle进程由kernel自动创建, 运行在内核态
# idle进程其pid=0，其前身是系统创建的第一个进程，也是唯一一个没有通过fork或者kernel_thread产生的进程。完成加载系统后，演变为进程调度、交换

- init process: ...项目模板?/部门模板?（所有用户进程的祖先进程）
# init进程由idle通过kernel_thread创建，在内核空间完成初始化后, 加载init程序, 并最终用户空间。是系统中所有其它用户进程的祖先进程。
# Linux中的所有进程都是有init进程创建并运行的。首先Linux内核启动，然后在用户空间中启动init进程，再启动其他系统进程。在系统启动完成完成后，init将变为守护进程监视系统其他进程。

- kthreadd process: 公司的项目管理部门（所有内核进程的祖先进程）（可以看到和init process一个主外一个主内）
# kthreadd进程由idle通过kernel_thread创建，并始终运行在内核空间, 负责所有内核线程的调度和管理
# 它的任务就是管理和调度其他内核线程kernel_thread, 会循环执行一个kthread的函数，该函数的作用就是运行kthread_create_list全局链表中维护的kthread, 当我们调用kernel_thread创建的内核线程会被加入到此链表中，因此所有的内核线程都是直接或者间接的以kthreadd为父进程
```

为啥kthreadd process是ks中管理线程的，是不是因为init是用来管理ks中进程的


### 几种IPC (pipeline, MQ, share mem, sema, socket)



如果把进程类比为公司项目，那IPC可以理解为不同项目之间的交流和协作

```yaml
- IPC: 很显然，IPC就是公司项目之间交流。显而易见的，公司不同项目组之间的关系也不同，或近或远，所以这些IPC方案也就很易于理解了。
# 但是需要注意的是，所有process都运行在用户空间，所以IPC必须通过内核空间(因为userspace不能直接访问硬件资源)
# 需要注意的是，内核空间和用户空间之间通信，同样使用这几种IPC方法
# 不同的 IPC 方法适用于不同的场景和需求。例如，信号适用于进程间的简单通知和事件处理，共享内存适用于高性能的数据共享，管道适用于父子进程之间的通信，套接字适用于网络通信等。
- signal: x
# signal types? # (TERM, STOP, Interrupt, (LGN, Cont)) (real-time signals, standard signals)
# How to create a custom signal in kernel?
# (USR2(graceful upgrade), WINCH(close worker one by one), QUIT(close master)) How to rollback? (HUP, QUIT)


- pipeline: 瀑布式开发（将整个软件开发过程分成多个阶段，往往是上一个阶段完全做完，才将输出结果交给下一个阶段）
# 所谓的pipeline，就是在两个进程之间建立一条单向的通道，其实是一段缓存，它会将前一个命令的输出，作为后一个命令的输入。所以问题也是显然的，性能较差、也不支持太大数据量。
- mq: 发邮件
# 消息队列有点儿像邮件，发送数据时，会分成一个一个独立的数据单元，也就是消息体，每个消息体都是固定大小的存储块，在字节流上不连续。
- share memory: 共享会议室（这个就很好理解了，不同于mq这种两个项目组之间还要发送消息，共享内存相当于两个项目组开辟一个办公室，所有需要同步的数据都放到这个办公室里）
# 前面咱们讲内存管理的时候，知道每个进程都有自己独立的虚拟内存空间，不同的进程的虚拟内存空间映射到不同的物理内存中去。这个进程访问 A 地址和另一个进程访问 A 地址，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。
# 但是，咱们是不是可以变通一下，拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去。
- sema: 避免“共享办公室”数据出错的方法，某个时间点，只能有一个项目组来访问
# 马哥说：“共享内存也有问题呀。如果两个进程使用同一个共享内存，大家都往里面写东西，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。”
# 张昭说：“当然，和共享内存配合的，有另一种保护机制，使得同一个共享的资源，同时只能被一个进程访问叫信号量。”
# 信号量其实是一个计数器，主要用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。
- socket:

```

```markdown
假设有一个大型公司，由多个部门组成，每个部门都负责不同的任务。这些部门需要相互合作和交流以完成工作。在这个公司中，我们可以使用以下 IPC 机制：

1. 管道（Pipe）：类似于公司内部的电话系统。每个部门都可以通过电话与其他部门进行直接通话。这种通信方式只能在亲密的部门之间进行，并且是单向的。

2. 共享内存（Shared Memory）：类似于公司内部的公共白板。多个部门可以在白板上读写信息以进行共享。这种通信方式可以实现高速数据传输，但需要注意协调和同步问题。

3. 信号量（Semaphore）：类似于公司内部的会议室预约系统。部门可以申请使用会议室，并在使用完毕后释放。这种通信方式可以用于同步和互斥操作。

4. 消息队列（Message Queue）：类似于公司内部的邮件系统。部门可以将消息放入邮件箱中，其他部门可以从中读取。这种通信方式可以实现异步通信，并支持多对多的通信模式。

5. 套接字（Socket）：类似于公司内部的互联网连接。部门可以通过套接字建立网络连接，进行跨网络的通信。这种通信方式可以实现分布式系统的协作。
```

- pipeline：电话
- 共享内存：共享会议室
- sema：预约会议室
- MQ：邮件
- socket：内网IM

### ns, cgroup


```yaml
- namespace: docker相当于公司一级项目内部创业，那么可以把ns看作是独立办公区域
# 相当于给每个项目组分配的独立办公区域。这些办公区域让项目组感觉自己就像是在一个独立的子公司里工作，拥有自己的网络（`network` namespace）、自己的文件系统（`mount` namespace）、自己的进程管理（`pid` namespace）等。这样，不同的项目组之间不会相互干扰，每个项目组都可以专注于自己的工作。
# namespace, types? # uts, user, mount, pid, network, IPC

- cgroup: 该内部创业的项目组有了独立预算
# 就像是给每个项目组设定的资源预算。这个预算规定了项目组可以使用的办公空间大小（内存限制）、能源消耗（CPU时间）、以及其他办公资源（如磁盘I/O限制）。通过这种方式，公司（内核）确保所有项目组都能公平地使用资源，而不会因某个项目组的过度消耗而影响其他项目组。
# cgroup (task, control group, hierarchy, subsystem)
# cgroup, subsystem? # cpu, cpuacct, cpuset, memory, blkio, devices, net_cls, freezer, (huge_tlb, perf_event, ...)

# ***namespace是对文件系统、进程pid、主机名、用户uid、设备、IPC以及网络的隔离，cgroup就是对CPU、内存、PID、磁盘IO等资源进行限制，这也是为啥我们说“ns是看起来隔离，cgroup是用起来隔离”。那就很简单了，ns里的那几个不就正好对应pid, uts(unix timesharing system), user, ipc, mount这些吗，cgroup则更简单了就是各种资源嘛，cpu(cpuacct(cpu account), cpuset), memory, blkio(block io), devices之类的，都是用来限制进程资源的。***

# ***换个角度来看，按照“docker容器的本质是进程”的说法，所谓的ns和cgroup提供的隔离，本质上来说就是给该进程提供了独立的各种ns和cgroup，是吗？正如cgroup原本的名字“process containers”本身所暗示的。如果我们还拿之前“开公司”来举例，普通的进程只是公司下面的一个项目组（普通进程只有用来标识进程的那些field（或者说进程本身相关的数据）是独享的（比如进程状态、fd、进程本身的内存空间），其他资源如kernel, filesystem, network, devices之类的都是共享的），docker（容器化的进程）则拥有真正独立的（体现在这部分资源不参与整个公司资源调度，是固定开支。内部也可以拥有idle进程、init进程、kthreadd进程这些一二三号位）。我的理解对吗？***

```

cgroup

```markdown
- `task`在 cgroup 中，任务就是一个进程。
- `control group`cgroup 的资源控制是以控制组的方式实现，控制组指明了资源的配额限制。进程可以加入到某个控制组，也可以迁移到另一个控制组。
- `hierarchy`控制组有层级关系，类似树的结构，子节点的控制组继承父控制组的属性 (资源配额、限制等)。
- `subsystem`一个子系统其实就是一种资源的控制器，比如 memory 子系统可以控制进程内存的使用。子系统需要加入到某个层级，然后该层级的所有控制组，均受到这个子系统的控制。
```




### cgroup


```markdown
在 2006 年，Google 的工程师（ Rohit Seth 和 Paul Menage 为主要发起人） 发起了这个项目，起初项目名称并不是cgroups，而被称为进程容器（process containers）。在 2007 年cgroups代码计划合入Linux 内核，但是当时在 Linux 内核中，容器（container）这个词被广泛使用，并且拥有不同的含义。为了避免命名混乱和歧义，进程容器被重名为cgroups，并在 2008 年成功合入 Linux 2.6.24 版本中。cgroups目前已经成为 systemd、Docker、Linux Containers（LXC） 等技术的基础。
```




### false sharing


false sharing 就是因为多个线程之间频繁地竞争cache line（导致cache line不断失效和重新加载的现象），导致额外的缓存同步开销，降低了并发性能。可以类比为团队间的不必要竞争或冲突。例如，如果两个部门为了争夺有限的资源或优先级而不断进行内部竞争，这可能会导致效率低下和资源浪费。为了避免这种情况，公司需要优化内部沟通和资源分配机制，确保团队协作而不是相互竞争。

一句话总结就是，多个线程同时访问共享内存中存在某一个cache line的不同变量，如果某个线程修改其中一个变量，那么整个cache line都会失效，其他线程也需要重新加载整个cache line，这个就是false sharing。命名还是很到位的，本身是为了能够sharing memory，但是如果cache line失效必须重新加载的话，那不就false了吗？




### signal


信号是用来操作进程的方式，这里需要注意的是，*并不仅仅是进程之间协作，而是进程和内核、进程内，都是通过信号，进行协作和通信的*

信号有很多种，平时也用不到，也没必要去死记硬背。只要知道就这么几种，终止信号、中断信号、暂停信号、继续信号、忽略信号






### waitqueue 和 workqueue 分别是啥？



```yaml
- waitqueue: 项目中的“等待状态”或“依赖管理”。例如，一个项目可能需要等待关键资源到位、审批完成或市场条件成熟才能继续进行。通过有效的项目管理工具和沟通机制，公司可以监控这些依赖关系，并在条件满足时迅速推进项目。
# 是一种同步机制，允许进程或线程在某个条件不满足时挂起，直到被另一个进程或线程唤醒。
# waitqueue 用于实现进程的阻塞和唤醒机制。当进程需要等待某个条件满足时，它会将自己放入等待队列并进入睡眠状态，放弃CPU控制权。当条件满足时，内核会通过wake up操作唤醒等待队列中的进程，使其继续执行。等待队列允许多个进程等待同一个事件，并且可以灵活地添加或删除等待队列中的进程。

- workqueue: 工作队列可以类比为“任务调度”或“待办事项列表”。公司可以使用这种机制来安排和优化日常工作流程，确保任务根据优先级和资源可用性得到合理分配。这有助于提高工作效率，确保重要任务得到及时处理，同时也不忽视那些需要更多时间来完成的任务。
# 工作队列（Workqueue）是一种延迟执行机制，用于将任务推迟到稍后执行，通常用于异步处理和负载均衡。

- cmwq: Cmwq可以类比为“项目里程碑跟踪系统”。公司可以设置一系列的里程碑，每个里程碑代表项目的一个关键阶段或目标的完成。
# cmwq 是一个特定于Linux内核的机制，用于管理完成（completion）事件的工作队列
# What is cmwq (Concurrency Managed Workqueue)? What problems were resolves?
```

workqueue是一种bottom half机制，用于在将来某个时间点执行函数，强调的是动态的概念，即工作（work）是重点，而队列（queue）是其次。工作队列运行在进程上下文，可以睡眠，适合处理不是很紧急事件的回调方式。而等待队列则是一种任务队列，重点在于队列本身，它允许进程在某个事件上睡眠等待，直到该事件发生才被唤醒继续执行。

**CMWQ通过引入worker pool的概念来提高并发性。在CMWQ中，所有的workqueue共享系统中的worker pool，而不是每个workqueue创建自己的worker thread。这样，当一个worker thread因为处理某个work而阻塞时，worker pool可以创建新的worker thread来处理其他work，从而解决了并发问题。此外，CMWQ还通过动态创建和销毁worker thread来适应工作负载，进一步优化了资源使用和并发性能。



### PCB, TCB



```yaml
- PCB (process control block): 公司一级项目的项目经理
# registers
- TCB (Thread Control Block): ...
# TCB结构是一个数据结构，它包含了线程执行所需的所有上下文信息。这些信息通常包括线程ID（TID）、栈指针（SP）、程序计数器（PC）、线程状态、以及线程的所有寄存器值等。
# TCB 的主要作用是保存和管理线程的上下文信息，以便在线程切换时能够正确地恢复和执行线程。操作系统通过操作 TCB 来实现线程的创建、调度、切换和销毁等功能。

# TCB中有哪些register?
# TCB中通常包含了多个寄存器的值，以确保线程上下文的完整保存和恢复。这些寄存器包括程序计数器（PC）、栈指针（SP）、基址寄存器（BR）、中断返回寄存器（IR）、调试寄存器（DR）、状态寄存器（SR）和通用寄存器（GPR）。

# - TCB (mapping) similar with TSS, PCB
# - What's TCB? # kernel control thread, store and manage thread's context
# - What's the TCB struct? # tid, SP, PC, state of thread, thread's register value
# - TCB中有哪些register? # PC, SP, BR, IR, DR, SR, GPR

#   - What's thread safe? # race condition, data race
#   - Why should use multi-thread, other than multi-process? Compare these.
#   - How to implement thread in user space?

```


```markdown
诶，这么说的话，线程的 TCB 是不是类似进程的 TSS？都是用来在上下文切换的时候，来存储切换之前映射关系的？

---

是的，线程的 TCB（线程控制块）和进程的 TSS（任务状态段）在某种程度上可以类比。

TCB 是用来存储线程的上下文信息的数据结构，包括寄存器状态、堆栈指针、调度信息等。在线程切换时，操作系统会保存当前线程的上下文信息到其对应的 TCB 中，并加载下一个线程的上下文信息，以实现线程的切换。

类似地，TSS 是用来存储进程的上下文信息的数据结构，包括寄存器状态、堆栈指针、段选择子等。在进程切换时，操作系统会保存当前进程的上下文信息到其对应的 TSS 中，并加载下一个进程的上下文信息，以实现进程的切换。

虽然 TCB 和 TSS 都用于存储上下文信息并在切换时进行加载，但它们的具体实现和功能可能有所不同。TCB 通常是线程库或操作系统内核实现的数据结构，用于管理线程的状态和调度，而 TSS 是由处理器提供的硬件支持，用于管理任务的上下文切换。

总的来说，TCB 和 TSS 都是用来存储上下文信息的数据结构，用于在上下文切换时保存和加载相关的状态。它们在不同的上下文切换场景中发挥着类似的作用，但具体实现和使用方式可能有所不同。

```






### 怎么查看一个进程究竟占用多少内存？为啥RSS在使用pmap和ps时不同?




- USS(`Unique Set Size进程独占的物理内存（不包含共享库占用的内存）`) 是一个进程所占用的私有内存。即该进程独占的内存。*USS 是非常非常有用的数据，因为它反映了运行一个特定进程真实的边际成本（增量成本）*。当一个进程被销毁后，USS 是真实返回给系统的内存。当进程中存在一个可疑的内存泄露时，USS 是最佳观察数据。
- PSS(`Proportional Set Size实际使用的物理内存（比例分配共享库占用的内存）`) 与 RSS 不同，它按比例表示使用的共享库，例如：如果有三个进程都使用了一个共享库，共占用了 30 页内存。那么 PSS 将认为每个进程分别占用该共享库 10 页的大小。*PSS 是非常有用的数据，因为系统中所有进程的 PSS 都相加的话，就刚好反映了系统中的总共占用的内存*。而当一个进程被销毁之后，其占用的共享库那部分比例的 PSS，将会再次按比例分配给余下使用该库的进程。这样 PSS 可能会造成一点的误导，因为当一个进程被销毁后，PSS 不能准确地表示返回给全局系统的内存（the memory returned to the overall system）。
- RSS(`Resident Set size常驻内存（包含共享库占用的内存）`) 是一个进程在 RAM 中真实存储的总内存。但是 RSS 还是可能会造成误导，因为它仅仅表示该进程所使用的所有共享库的大小，它不管有多少个进程使用该共享库，该共享库仅被加载到内存一次。所以 RSS 并不能准确反映单进程的内存占用情况。
- VSS(`Virtual Set Size虚拟内存（包含共享库占用的内存）`) 单个进程全部可访问的地址空间，其大小可能包括还尚未在内存中驻留的部分。对于确定单个进程实际内存使用大小，VSS 用处不大。

*结论，VSS>=RSS>=PSS>=USS*

RSS和VSS都没啥用，USS和PSS比较有用，

我们还是拿公司举例，进程就是项目组，毫无疑问这些就是该项目组占用的工作区空间。

唯一集、比例集、常驻集、虚拟集

房屋面积=套内面积+公摊面积

USS和PSS我能理解，USS在我看来就是该项目组本身占用的空间，PSS也很好理解，就像我们在公司一样，肯定会有很多公共区域如会议室、休息区、卫生间、茶水间、健身房之类的，PSS就是按照多少个项目组，按比例分配一下，计算出来的占用空间。换句话说，USS就是套内面积，PSS就是加上公摊面积之后的房屋面积。那么RSS和VSS按照这个理解，可以类比成什么呢？

RSS和VSS都可以理解为这个项目组会使用到的上面说的全部公共空间，甚至其中一些空间，该项目组并不会使用到，比如说可能该项目组可能并不会真正用到健身房，但是也要算在内。VSS则不仅如此。

说回golang，golang因为是静态链接，所以没有共享库。换句话说，就是没有公摊面积，所以其套内面积，就等于住房面积。






## 内存




### z

kernel 内存管理


```markdown
当然可以！Linux 内存管理是操作系统对内存资源的管理和分配。Linux 使用虚拟内存的概念，将物理内存和磁盘空间结合起来，以提供更大的内存空间和更灵活的内存管理。

Linux 内存管理的核心任务包括：

1. 内存分配：Linux 使用页面（Page）作为内存管理的基本单位。当进程需要内存时，它会向操作系统请求一定数量的页面。操作系统负责分配物理页面，并建立虚拟内存与物理内存之间的映射关系。

2. 页面置换：当物理内存不足时，Linux 会使用页面置换算法将一些页面从物理内存换出到磁盘，以腾出空间给新的页面使用。常见的页面置换算法包括最近最少使用（LRU）和时钟（Clock）算法。

3. 内存回收：当进程释放内存时，Linux 会回收这些页面，并将它们重新放入可用内存池中，以便其他进程使用。

4. 内存映射：Linux 支持将文件映射到内存，以便进程可以像访问内存一样访问文件。这种机制被广泛用于加速文件访问和共享内存。

5. 虚拟内存管理：Linux 使用虚拟内存机制将进程的地址空间与物理内存分离开来。每个进程都有自己的虚拟地址空间，而不必关心物理内存的细节。这为进程提供了更大的内存空间和更高的安全性。

这只是对 Linux 内存管理的简要介绍，实际上涉及的内容非常广泛，包括内存段、内存映射、页面回写等。如果你对具体的内存管理问题有进一步的需求，请告诉我，我将竭力为你提供更详细的信息和相关代码。
```



### DMA


```markdown
如果我们对于 I/O 的操作，都是由 CPU 发出对应的指令，然后等待 I/O 设备完成操作之后返回，那 CPU 有大量的时间其实都是在等待 I/O 设备完成操作。

但是，这个 CPU 的等待，在很多时候，其实并没有太多的实际意义。我们对于 I/O 设备的大量操作，其实都只是把内存里面的数据，传输到 I/O 设备而已。在这种情况下，其实 CPU 只是在傻等而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过 CPU，实在是有点儿太浪费时间了。

因此，计算机工程师们，就发明了 DMA 技术，也就是直接内存访问（Direct Memory Access）技术，来减少 CPU 等待的时间。
```

DMA是用来减少CPU等待时间的，其实可以理解为DPDK之于普通的kernel network


```markdown
其实 DMA 技术很容易理解，本质上，DMA 技术就是我们在主板上放一块独立的芯片。在进行内存和 I/O 设备的数据传输的时候，我们不再通过 CPU 来控制数据传输，而直接通过DMA 控制器（DMA Controller，简称 DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。

DMAC 最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。
```

co-processor

```markdown
除此之外，DMAC 其实也是一个特殊的 I/O 设备，它和 CPU 以及其他 I/O 设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。
```




### zero-copy

```markdown
## 零拷贝


它的主要目的是减少数据拷贝的次数，以提高 I/O 操作的效率。通过使用零拷贝技术，可以避免将数据从内核空间复制到用户空间，从而减少了内存和 CPU 的开销

数据的最终处理还是在用户空间，但是在此之前内核必须要对其进行验证、解析、分发等操作。

### 啥是 linux 的拷贝？为啥需要零拷贝？啥是零拷贝？(直接复制的)

拷贝是 linux 中的标准 io 接口的基本数据拷贝的操作，而这一操作会导致数据在 OS 内核地址空间的缓冲区和应用程序地址空间定义的缓冲区之间进行传输

---

- 传统的 IO 操作有性能问题，我们通过`零拷贝`进行改进
- *零拷贝的意思是避免数据拷贝，而不是没有拷贝*

  具体来说就是：

- 避免操作系统内核缓冲区之间进行数据拷贝操作。
- 避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。
- 用户应用程序可以避开操作系统直接访问硬件存储。
- 数据传输尽量让 DMA 来做。
- 避免不必要的系统调用和上下文切换。
- 需要拷贝的数据可以先被缓存起来。
- 对数据进行处理尽量让硬件来做。

---

  零拷贝的方法 (没看懂)

- `直接 I/O`，应用程序直接访问硬件存储，内核只是辅助数据传输，*避免`内核地址空间`的缓冲区和`用户应用程序地址空间`的缓冲区之间的拷贝*
- `DMA+mmap`，数据先通过 DMA 拷贝到内核缓冲区，然后，应用程序和 OS 共享这个缓冲区，这样，内核和应用程序存储空间就不需要再进行任何数据拷贝操作了
- `sendfile`，利用 DMA 将文件中的数据，拷贝到 OS 内核缓冲区，然后将数据拷贝到与 socket 相关的内核缓冲区中，然后，DMA 将数据从内核 socket 缓冲区中拷贝到协议中
- `DMA 辅助的 senfile`，之前我们把页缓存的数据拷贝到 socket 中，实际上，只需要把缓冲区 fd 传到 socket 缓冲区，再把数据长度传过去，这样 DMA 控制器直接将页缓存中的数据打包发送到网络中就可以了

---

怎么用 sendfile 实现零拷贝？

### 什么是“优化零拷贝”？有哪些优化零拷贝的方法？

以上几种零拷贝方式都是减少数据在用户空间和内核空间拷贝技术实现的，但是有些时候，数据必须在用户空间和内核空间之间拷贝；这种情况下，我们只能针对数据在用户空间和内核空间拷贝的时间点上解决问题。

- `写时复制 COW`
- `缓冲区共享`



### COW 是什么？写时复制和强制分裂？具体聊聊 linux 中的 COW？

- COW 是一种通用的优化策略，`linux 的 vmm 虚拟内存管理器`，`数据库的很多应用如 snapshot 快照`，以及`各种开发语言`都使用到这种策略，多个调用者在读取时都共享同一份资源，用开发语言举个例子，假设有变量 a 和 b，a=b 做了赋值，两个变量会指向同一个内存地址，该对象的 rc 就是 2；当我们对 b 重新赋值，就发生了`写时复制`，在内存中新建一个内存地址，两个对象的 rc 都是 1
- linux 中 cow 的 fork()；创建一个子进程，内核只为子进程创建虚拟空间，不分配物理内存，和父进程共享物理空间；当父进程中有更改相应段的行为发生时，才为子进程分配物理空间
- cow 在需要写入时，数据才会被复制；没有数据写入时，fork 的开销实际只是复制父进程的页表以及给子进程创建唯一的进程描述符；有数据要写入时，会将要改变的数据页复制给子进程
- 一般把这种被共享访问的页面标记为只读。当一个 task 试图向内存中写入数据时，内存管理单元（MMU）抛出一个异常，内核处理该异常时为该 task 分配一份物理内存并复制数据到此内存，重新向 MMU 发出执行该 task 的写操作
- rc 存在的意义，就是为了 cow 可以正常工作，从而实现对内存的优化

```

需要理解 ***“使用zc可以避免将数据从内核空间复制到用户空间，从而减少了内存和 CPU 的开销”***

因为当数据从磁盘或网络传输到内核空间时（也就是正常的IO操作），传统的方式是将数据复制到用户空间，然后再进行处理。这个过程中涉及了两次数据拷贝，一次是从内核到用户空间，另一次是从用户空间到其他地方。

但是，零拷贝 COW 就是要绕过这个繁琐的过程。它通过让多个进程共享同一块内存区域，在进行写操作时才进行数据的复制。也就是说，当有进程要修改数据时，才会将数据复制到新的内存区域，其他进程仍然可以共享原始数据的内存区域。这样就避免了不必要的数据拷贝，提高了效率。

那么数据为什么要传输到内核空间呢？

因为数据的最终处理还是在用户空间，但是在此之前内核必须要对其进行验证、解析、分发等操作（处理完之后再从ks发到us）

另外，需要注意的是，kernel 提供了一些COW的机制，如 sendfile 和 splice 系统调用，可以在某些情况下实现COW操作。但并不是所有的 I/O 操作都可以使用这些机制，具体取决于应用程序的需求和操作系统的支持。***这就是为啥虽然zc很好，但是kernel没有将其作为默认的io操作，而只是一种优化方案***

```yaml
zero copy:
COW:
# COW和sendfile是一回事吗？
# COW（Copy-on-Write）是一种内存管理技术，用于优化数据的复制操作。当多个进程或线程共享同一块内存时，它们最初都指向相同的内存页。当其中一个进程或线程试图修改该内存页时，操作系统会创建该页的副本，并将修改应用于副本，而不是原始页面。这样做的好处是避免了不必要的数据复制，节省了内存和时间。
# sendfile是一个系统调用（在某些操作系统中），用于在文件之间直接传输数据，而无需将数据从内核空间复制到用户空间，再从用户空间复制到网络套接字。它通常用于高效地将文件内容发送到网络连接中，例如在Web服务器中传输文件给客户端。sendfile的使用可以减少数据复制的次数，提高数据传输的效率。
# 简而言之，COW是一种内存管理技术，用于优化数据的复制操作，而sendfile是一个系统调用，用于在文件之间直接传输数据。它们是不同的概念和用途。


# COW 是什么？写时复制和强制分裂？具体聊聊 linux 中的 COW？
# 啥是 linux 的拷贝？为啥需要零拷贝？啥是零拷贝？零拷贝的方法？怎么用 sendfile 实现零拷贝？
# 什么是“优化零拷贝”？有哪些优化零拷贝的方法？
```



### mmap


```yaml
mmap: mmap的本质是kernel的syscall，只不过可以用来作为storage engine而已
# mmap是一个syscall，用于将文件或其他对象映射到进程的地址空间。通过mmap，进程可以像访问普通内存一样访问文件内容，这通常用于高效地读取和写入大型文件。mmap可以创建一个文件的内存映射，这个映射可以用于读写操作，而且映射可以是共享的（多个进程可以共享同一映射），也可以是私有的（对映射的修改不会反映到原始文件中）。mmap还支持将匿名内存映射到进程地址空间，这通常用于动态分配大块内存。

madvise:
# madvise是一个syscall，用于给内核提供关于进程如何使用内存的建议。通过madvise，进程可以告诉内核它对特定内存区域的使用模式，例如，是否频繁访问、是否不常访问、是否应该被预读到内存中等。内核会根据这些建议来优化内存管理策略，例如，可能会根据建议来决定是否将内存页换出到磁盘上，或者是否预读文件内容。需要注意的是，madvise只是提供建议，内核可能会忽略这些建议，具体行为取决于内核的内存管理策略和其他系统状态。
```


mongodb和boltDB都把 mmap 作为 SE 使用

总结来说就是，**用 mmap 作为 SE 最核心的问题，就是无法做到对内存容量的精准控制。**

这部分内容可以和 MongoDB SE 结合来看






### 虚拟内存



```markdown
虚拟内存是现代操作系统中的一项关键技术，它允许程序在运行时使用比物理内存更多的内存空间。为了管理虚拟内存，操作系统需要决定哪些内存页应该保留在物理内存中，哪些可以暂时移出到磁盘上的交换空间（swap space）。当CPU需要访问的内存页不在物理内存中时，会发生页面错误（page fault），操作系统必须从磁盘中将该页调入内存。同样，当物理内存不足以容纳所有需要的页时，操作系统必须选择一些页将其移出内存，以便为新的页腾出空间。这个过程称为页替换（page replacement）。

页面置换：在地址映射过程中，若在页面中发现所要访问的页面不再内存中，则产生缺页中断(page fault)。当发生缺页中断时操作系统必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。

操作系统虚拟内存中的四种典型页替换算法

OPT (Optimal Page Replacement), LRU, FIFO, Clock
```

*可以直接把页置换看作是虚拟内存的scheduler*

```yaml
memory: CPU就是员工，memory就是写字楼（办公区域）（如果CPU是工人，内存就是工厂）

# virtual memory

virtual memory: 用来实现不同项目之间保密
# 用来管理用户空间
# 对用户空间，我们用`虚拟内存`对内存进行管理；对于内核，使用`slab`或者`vmalloc`的方式对内存进行管理
NUMA: 就像公司里不同部门也有各自专有的工作区域一样，NUMA 就是这种 CPU 和内存一一对应的架构。而 SMP 则对应那种不同部门混在一起工作，这是很不合理的。
# locality(local mem, non-local mem), swap
NUMA Affinity:
# CPU NUMA Affinity, Device NUMA Affinity(DMA Buffer, Interrupt)


swap:
# 虚拟内存和 swap 的关系？虚拟内存和 swap 是两回事，实际上 swap 只是虚拟内存引申出的一种技术而已，*操作系统一旦内存不足，为了腾出内存空间存放新数据，就会把当前物理内存中的数据放到 swap 交换分区里，需要使用的时候再取出*。swap 是磁盘上的一块区域，当内存吃紧时，就会将内存中不常访问或者空闲的页的部分，保存到 swap 上，这样系统就有更多的物理内存能为进程服务。当系统需要访问 swap 的内容时，就加载到内存上。

# swap 解决了什么问题？
# swap 解决了内存不充裕的情况下，导致服务不可用的问题，swap 能够保证服务可用，但是性能相对会下降

# swap 有哪些缺点？
# 因为 swap 是存放在磁盘上的，所以磁盘读起来会比内存慢很多。牺牲了系统的性能保证了服务的可用性。

swap thrashing:
# swap 颠簸
# 需要注意的是，swap 的使用可能会带来性能问题，物理内存和 swap 频繁发生数据交换，被称为 swap 颠簸，一旦发生这种问题，需要明确是什么原因造成的

PRA(Page Replacement Algo):


#- 如何减少对swap空间的使用？
# 可以通过修改swappiness参数的值来减少对swap空间的使用。swappiness的值决定了系统使用swap空间的倾向。值越低，系统越倾向于使用物理内存，而不是swap空间。例如，可以通过执行命令sudo sysctl vm.swappiness=10来临时将swappiness设置为10。为了使这个设置永久生效，可以编辑/etc/sysctl.conf文件，并添加行vm.swappiness=10，然后重启系统。

#- swap空间的使用过高可能是什么原因？
# swap空间使用过高可能是由于系统内存资源紧张导致的。当系统内存不足时，为了满足新的内存分配请求，系统会通过直接内存回收和定期扫描的方式释放文件页和匿名页，将不常访问的数据换出到磁盘中的swap空间。此外，如果系统配置了swap，但在某些情况下（如大量文件操作或内存密集型操作）没有有效地管理内存，也可能导致swap使用率升高。

#- 如何区分系统性能下降是由于swap thrashing还是由于非swap的高I/O延迟造成的？
# 可以通过监控工具（如vmstat、iotop、sar等）来分析系统的性能指标。swap thrashing通常表现为大量的换入换出操作，可以通过vmstat中的si和so字段来观察。非swap的高I/O延迟可能是由于磁盘I/O请求处理缓慢，可以通过iotop或iostat来查看磁盘I/O的使用情况。如果I/O等待时间（例如vmstat中的wa字段）显著增加，可能是高I/O延迟的问题。

#- 如何监控和记录swap空间的使用情况？
# 可以使用free -h命令来查看当前的内存和swap空间使用情况。为了记录和监控swap空间的使用情况，可以使用sysstat工具包中的sar命令，它可以收集、报告或保存系统活动信息，包括内存和swap的使用情况。此外，/proc/vmstat文件也提供了关于虚拟内存系统的统计信息，可以用来分析swap活动。

#- 如何关闭swap空间？
# 可以通过swapoff命令来关闭swap空间。例如，sudo swapoff -a可以关闭所有的swap空间。如果需要重新启用swap空间，可以使用swapon命令，例如sudo swapon -a来重新启用所有配置的swap空间。需要注意的是，关闭swap空间可能会影响系统的稳定性和性能，特别是在内存资源紧张的情况下。

```

```markdown
第一，物理内存的管理，相当于会议室管理员管理会议室；

第二，虚拟地址的管理，也即在项目组的视角，会议室的虚拟地址应该如何组织；

第三，虚拟地址和物理地址如何映射的问题，也即会议室管理员如果管理映射表。
```

我们还是拿公司举例，意思是假如公司有 100 个工位，但是有 n 个项目，这 n 个项目还是同时运行的，如果不用虚拟内存，就会出现 1 个工位被 n 个项目反复修改，就出现错误了。使用虚拟内存之后，即使这 n 个项目都需要同一个工位（内存地址），但是老板（内核）会给不同的项目划分不同的区域（虚拟内存和物理内存的映射表），同一个工位实际上映射在不同的地方（物理内存地址）。

其实我们在日常生活中就是这么设计的，所以比较难理解之前“物理内存”是个什么情况。

其实上面的这个说法是有些问题的，日常工作时不同开发组是可见的，但是进程之间则不是，每个进程都认为整个写字楼都是自己独占的。实际上更合理的说法是，所有进程都想进入 21F-1 的办公室，在下面摁电梯时，身在写字楼顶层的高管会把这 n 个进程划分到不同的楼层、不同的办公室，但是这些进程自己认为自己都进入了 21F-1 办公室，也认为整个写字楼都是自己独占的。

---

这里的类比也跟我之前预想的一摸一样，内核空间就是高管所在的区域，写字楼的顶层。员工们则在写字楼的下面楼层办公。

内核不能直接访问和使用用户空间的内存地址，从而保护用户空间的数据，以及进程稳定性。



### 物理内存



#### kernel中是怎么把物理内存映射到虚拟内存的?


mmap(memory mapping), page table, virtual address

- PGD、P4G、PUD、PMD、PTE 四级页表是啥？

```markdown
虚拟内存是对于物理内存的抽象。支持虚拟内存的处理器，通过使用 `虚拟寻址` 这种间接方法来引用物理内存。

*处理器产生一个虚拟地址，在被发送到物理内存之前，这个地址被翻译成一个物理地址。*

- 物理寻址
- 虚拟寻址

地址翻译

大多数页表条目位于 L1 高速缓存中，但是 TLB 的页表条目的片上高速缓存，通常会消除访问在 L1 上的页表条目的开销。

- 结合高速缓存和虚拟内存
- 利用 TLB 加速地址翻译
- 多级页表端到端的地址翻译

---

- *因为虚拟内存是操作系统物理内存和进程之间的中间层，为进程隐藏物理内存，为进程提供了简洁易用的接口*
- 虚拟内存可以为进程提供独立的内存空间，并引入多层的页表结构，将虚拟内存翻译成物理内存，*进程之间可以共享物理内存减少开销，也能简化程序的链接、装载以及内存分配过程*
- 虚拟内存可以控制进程对物理内存的访问，隔离不同进程的访问权限，提高系统的安全性

```

这个就是虚拟内存和物理内存的映射




#### zone_type

```yaml
# - zone_type # ZONE_DMA(kernel thread), ZONE_NORMAL(user thread), (ZONE_HIGHMEM, ZONE_DEVICE)
# 内存 zone 分为四块，除了内核态和用户态分别访问的内存区域，还分为 ZONE_HIGHMEM 和 ZONE_DEVICE 这两个用来解决特殊问题的 zone，这两个对用户态线程都不可见。
```

```markdown

内核空间的区域由哪几部分构成？

- `DMA 内存区域`0-16mb 内存
- `NORMAL 内存区域`16-896mb 内存
- `HIGHMEM 内存区域`高端内存区域

---

`用户空间`里由低到高的地址，分别是？

- `代码段/只读段 (text section)`，*`可执行文件`代码的内存映射*，存放可执行文件的操作指令，可执行程序在内存中的镜像 (只读，不可写)
- `数据段 (data section)`*`可执行文件`的已初始化全局变量的内存映射*，可执行文件中已初始化全局变量，静态分配的变量和全局变量
- `BSS(bbs segment)`*`可执行文件`未初始化的全局变量*
- `heap` 用来存放进程运行时被动态分配的内存段，大小不固定。malloc 新内存加到 heap，free 时从 heap 释放
- `stack` 临时的局部变量，函数中定义的变量 (不包括 static)。函数被调用时，参数也会被压入发起调用的进程栈中，调用结束时，返回值也会被存放到栈。

```

heap, stack, bbs, data section, text section



#### vmalloc


```markdown
slab，slub，slob 是slab机制的三种不同实现算法。
注意，slab申请和分配的都是只针对内核空间，与用户空间申请分配内存无关。用户空间的malloc和free调用的是libc。

slab和buddy的关系？
1、slab的内存来自于buddy。slab相当于二级管理器。
2、slab和buddy在算法上，级别是对等的。

两者都是内存分配器，buddy是把内存条分成多个Zone来管理分配，slab是把从buddy拿到的内存，进行管理分配。

同理，malloc 和free也不找buddy拿内存。 malloc 和free不是系统调用，只是c库中的函数。
```



#### buddy-allocator


```yaml
buddy-allocator: 用来管理物理内存的内存分配算法。buddy allocator 就像是项目内部划分再给各种内部项目划分工作区域，一定是划分到最小粒度，也就是工位（也就是“空闲页放在 struct free_area 里面，每一页用 struct page 表示”），再规划出 11 种内部项目可以申请的档位（也就是“把所有的空闲页分组为 11 个页块链表，每个块链表分别包含很多个大小的页块，有 1、2、4、8、16、32、64、128、256、512 和 1024 个连续页的页块。最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。”）。这样项目内部需要创建子项目时，直接申请整块的工位区域（连续页的页块）就可以了。就不需要再一个一个申请工位了。其实跟我们在公司里也差不多，代入到实际生活就很好理解了。
```

ptmalloc, buddy allocator

感觉机制很像（都是通过把整块物理内存空间分割为不同大小的块，以减少内存碎片），但是确实不是一回事

ptmalloc是用户空间的，buddy allocator是内核空间的



#### TLB

```markdown
TLB: 是保存虚拟地址到物理地址的一个缓存，是 MMU 的一部分，存储在 CPU 和 CPU cache 之间，有效的减少了通过转换去获取物理地址的次数

TLB是MMU中的一个重要缓存结构，用于加速地址转换过程。TLB存储了最近使用的虚拟地址和物理地址之间的映射关系。当程序访问内存时，MMU首先会在TLB中查找对应的虚拟地址和物理地址映射，如果找到了匹配的映射，就可以直接使用TLB中的物理地址，避免了耗时的内存访问。如果在TLB中没有找到映射，MMU会通过一定的算法将虚拟地址转换为物理地址，并将新的映射关系存储到TLB中，以便下次使用。

总结起来，MMU是负责地址转换的硬件组件，而TLB是MMU中的一个缓存结构，用于存储最近的地址映射，以提高地址转换的速度。TLB可以减少对内存的访问次数，从而提高系统的性能。
```






## storage





## network




### select、poll、epoll 分别是什么？有什么区别？

```markdown
select

- `默认只能监听 1024 个 socket 连接`: 因为每次调用 select 都需要将进程加入到所有监视 socket 的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个 fds 列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 select 的最大监听数量，默认只能监听 1024 个 socket。
- `需要主动轮询`: select 监听 socket 时，不能知道哪个 socket 有可用数据，需要轮询判断。进程被唤醒后，用户程序并不知道哪些 socket 已经接收到数据，还需要遍历一次。

poll: 与 select 原理类似，只是使用`链表`实现，没有了 socket 限制。

epoll

- `没有 socket 数量限制`: 可以直接知道哪个 socket 有数据了，全面解决了 select 的问题。
- `线程安全`: epoll 是线程安全的，而 select 和 poll 不是。
- `mmap 共享内存`: epoll 内部使用了 mmap 共享了用户和内核的部分空间，避免了数据的来回拷贝。
- `基于事件驱动`: epoll_ctl 注册事件并注册 callback 回调函数，epoll_wait 只返回发生的事件避免了像 select 和 poll 对事件的整个轮询操作。
```



### epoll, Compare ET mode and LT mode?

你有急事打电话找人，如果对方一直不接，那你只有一直打，直到他接电话为止，这就是 lt 模式；如果不急，电话打过去对方不接，那就等有空再打，这就是 et 模式。

```markdown
`LT(level trigger)`
- socket 接收缓冲区不为空，有数据可读，读事件一直触发
- socket 发送缓冲区不满，可以继续写入数据，写事件一直触发

`ET(edge trigger)`
- socket 的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件
- socket 的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件

---

LT 和 ET 有什么区别？

- LT 模式的使用更简单，更符合正常认知，ET 只在状态变化时触发事件。
- ET 模式的效率比 LT 更高，因为可以减少 epoll 事件被重复触发的次数。
```


### 为什么 epoll 的效率高？


```markdown
- 因为基于`红黑树`、`双向链表`、`事件回调机制`
- 如果有 100 万用户同时与一个进程保持 TCP 连接，但是只有几十个 TCP 连接是活跃的（保持长连接但是未发送 TCP 包）
- 在这种场景下，select/poll 就会收集所有的连接（很没必要），且每次收集事件时，都会把这 100 万个 socket 传给 linux（`用户态内存`到`内核内存`的大量复制）。*select 和 poll 每次都需要遍历，所以时间复杂度是 O(n)，而 epoll 是基于回调的，每个 socket 上有事件发生时，就调用回调函数放到 epoll 的列表中，所以 epoll_wait 只需要读这个列表，时间复杂度是 O(1)*
```

epoll 和 select 实际上没有太大区别，只是 select 用数组标识 fds，epoll 用另一个 epollfd 来标识 fds；（*所以区别只在于 select 的 fds 每次等待都要设置进去，epoll 的 fds 是一开始就设置好的，等待时不需要重新指定*）

> epoll 并不仅仅基于红黑树。它还使用了其他数据结构来管理事件和回调函数，以及内核态和用户态之间的数据传输。epoll 使用红黑树来快速定位需要处理的文件描述符，然后进行相应的 I/O 操作。

epoll 的数据结构称为 eventpoll，是基于红黑树和双线链表的混合结构，使用红黑树来管理 fd，使用双线链表来管理事件和回调函数。也是这种混合结构的设计能让 epoll 多路复用处理并发事件。

***epoll 使用双向链表管理事件和回调函数，用红黑树管理 fd。而 mmap 是用来管理事件的内存映射，来实现不经过复制，让内核直接访问事件的，从而提高性能。***

---

```markdown

那 epoll 是基于双向链表来管理事件和回调函数吗？

---

具体来说，eventpoll 使用红黑树来高效地管理需要监听的文件描述符（file descriptor），以支持快速的插入、删除和查找操作。每个文件描述符都对应一个节点在红黑树中。

而双向链表则用于管理事件和回调函数。在每个红黑树节点中，都有一个链表，用于存储与该文件描述符相关的事件和回调函数。这样，当有事件发生时，可以快速遍历链表，执行相应的回调函数。

因此，可以说 epoll 在内核中使用了双向链表来管理事件和回调函数，而红黑树则用于高效地管理文件描述符。这种混合结构的设计使得 epoll 能够高效地处理大量的并发事件。

```

eventpoll - RBT node - DLL

意思是eventpoll中有大量RBT node，而每个node中是由DLL实现的？



### reactor, proactor, actor, CSP 有啥区别


```markdown
- 常见的线程模型有`reactor`/`proactor`/`actor`，*其中 reactor 和 proactor 都是通过共享内存来通信，而 actor 则是通过通信来共享内存*
- `proactor`就是把`reactor`的 IO 操作改为异步，异步接收，同时处理多个服务请求的事件驱动程序，性能更好，更能发挥 DMA 的优势，但是依赖 OS 调度来支持异步，主流 OS 对异步 IO 的支持都很有限，所以 proactor 几乎没有什么实际应用，不用看
```



### implement reactor using epoll





```markdown
reactor 模型具体有哪几种

- `单reactor单线程模型`，在 reactor 中处理事件，并分发，如果是`连接事件`就分发给 acceptor 处理，如果是`读写事件`和`业务处理`，就交给 handler，但是始终只有一个线程执行所有事件
- `单reactor多线程模型`，和`单reactor单线程模型`的区别在于，*reactor 只处理`连接事件`和`读写事件`，`业务处理`交给`线程池`处理*
- `多reactor多线程模型`，*把 reactor 线程池拆成了`主reactor`和`从reactor`两部分，`主reactor`只负责维护连接，`从reactor`处理读写事件，`业务处理`还是交给`线程池`处理*，具体来说，`从reactor`有多个，每个`从reactor`都在一个独立线程中执行，`从reactor`会去执行一些耗时的 IO 操作，用多线程更能发挥 CPU 的多核

```

```markdown
reactor 三种模式形象比喻

reactor 餐厅的接待员和服务员，或者处理工单，餐厅一般有接待员和服务员，接待员负责再门口接待顾客，服务员负责全程服务顾客。reactor 的三种线程模型可以用接待员和服务员类比。

- 单 reactor 单线程模型：接待员和服务员是同一个人，一直为顾客服务。适合客流量较少的情况。
- 单 reactor 多线程模型：一个接待员，多个服务员。客流量大，一个人忙不过来，由专门的接待员在门口接待顾客，然后安排好桌子后，由一个服务员一直服务，一般每个服务员负责几张桌子。
- 多 reactor 多线程模型：多个接待员，多个服务员。这种就是客流量太大了，一个接待员忙不过来。

```

总结一下，reactor的这几种模式，

这么说还是太笼统了，如果我们继续用餐厅的这个例子来解释epoll, poll, select

***相比于epoll，reactor “reactor 是对 epoll 的一种封装 (优化)，把 epoll 这种对 io 的管理，转化为对事件的管理”，具体来说就是“使用多路复用 io 模型处理 io 事件，让 reactor 将相应 io 事件和业务处理分开，一个或多个线程来处理 IO 事件，然后将就绪得到事件分发到业务处理 handlers 线程去异步非阻塞处理。”***

我说说我对这段话的理解，产生了一个疑惑

“既然epoll也用了event-table来管理fd和event，通过调用epoll_wait的syscall来监听event。这个不也是reactor所谓的“把对io的管理，转化为对event的管理”吗？”

确实，那么epoll和reactor有啥区别呢？

这里的问题在于“把对io的管理，转化为对event的管理”这句话太含糊了。

reactor其实就是某种对epoll的DI

正如这段所说

```markdown
回想一下普通函数调用的机制：程序调用某函数，函数执行，程序等待，函数将结果和控制权返回给程序，程序继续处理。reactor反应堆，是一种事件驱动机制，和普通函数调用的不同之处在于：应用程序不是主动的调用某个 API 完成处理，而是恰恰相反，reactor逆置了事件处理流程，应用程序需要提供相应的接口并注册到 reactor上，如果相应的事件发生，reactor将主动调用应用程序注册的接口，这些接口又称为“回调函数”。

说白了，reactor就是对epoll进行封装，进行网络IO与业务的解耦，将epoll管理IO变成管理事件，整个程序由事件进行驱动执行。就像下图一样，有就绪事件返回，reactor：由事件驱动执行对应的回调函数；epoll：需要自己判断。
```

为啥说是DI呢？epoll原来是

```markdown
reactor 模型是什么？

- *reactor 是一种线程模型，是 NIO 实现多路复用的一种模式*
- *reactor 用`IO复用`+`线程池`解决 BIO 的问题*，避免了 bio 的TPC这种开销高、效率低的线程模型
    - `IO复用`，多个连接共用一个阻塞对象，不用等待所有连接
    - `线程池`，避免为每个连接创建线程
```

```markdown
reactor 模型的工作原理？

- reactor 的实现非常简单，使用同步 io 模型，主要特点是利用 epoll 监听 fd 是否有响应，及时将客户连接信息放到一个队列里，epoll 和队列都是在主线程中，由子线程来接管各个 fd，对 fd 进行下一步操作。包括连接和数据读写。
- `reactor`把 IO 事件分配给对应 handler 处理
- `acceptor`处理客户端连接事件
- `handler`处理非阻塞任务
```

那我们把内容再延伸一些，怎么用上面这个餐厅的例子来类比proactor和actor呢？






### iptables


在这个比喻中，可以类比为公司的安保部门。这个部门负责监控和控制进入和离开各个办公区域的人员和物资（网络流量）。安保部门（iptables）通过设置各种规则来确保只有经过授权的人员可以进入特定区域，同时也控制着哪些区域之间可以互相访问。例如，安保部门可能会设置规则，只有来自特定供应商的物资才能进入仓库区域，或者只有持有特定通行证的人员才能进入研发实验室。

如果我们把内核空间比做公司核心部门，用户空间比做公司外围部门，那么 netfilter 可以看作是公司的安全监控系统，它负责监视和处理数据包的流动。过滤规则就像是安保部门下达的指令，用于决定哪些数据包可以通过，哪些需要被阻止。

***可以把四表理解为公司的部门区域，五链理解为动线。***

iptables 为了便于管理，所以按照不同的目的组织了多张 表 ；每张表中又包含了很多预定义的 链；每个链中包含着顺序遍历的 规则；这些规则中又定义了动作的匹配规则和 目标。

iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。


---


hook, (PREROUTING, INPUT, FORWARD, OUTPUT, POSTROUTING)

五链：*linux 的 netfilter 在内核协议栈的各个重要关卡埋下了五个钩子，每个钩子都对应一系列规则，以链表的形式存在，所以称为五链;* 当网络包在协议栈中流转到这些关卡的时候，就会依次执行在这些钩子上注册的各种规则，进而实现对网络包的各种处理。

*感觉有点类似于前置中间件，netfilter 通过 hook 函数在 PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUTING 这五个阶段*

```markdown
raw 用于配置数据包，raw 中的数据包不会被系统跟踪。
filter 是用于存放所有与防火墙相关操作的默认表。
nat 用于 网络地址转换 （例如：端口转发）。
mangle 用于对特定数据包的修改（参考损坏数据包 ）。
security 用于强制访问控制 网络规则。
```


iptables四表五链

```markdown
raw 表         确定是否对该数据包进行状态跟踪
mangle 表      为数据包设置标记
nat 表         修改数据包的源、目标IP地址、端口
filter 表      确定是否放行该数据包

---

INPUT          处理入站数据包
OUTPUT         处理出站数据包
FORWARD        处理转发数据包
POSTROUTING    在进行路由选择后处理数据包
PREROUTING     在进行路由选择之前处理数据包
```






## 《趣谈 Linux 操作系统》读书笔记



